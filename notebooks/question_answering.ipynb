{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b46b5f",
   "metadata": {},
   "source": [
    "<h1> Question Answering - Code Generation </h1>\n",
    "<br>\n",
    "\n",
    "Before starting, please make sure this notebook is using **conda_python3** kernel from the top right!\n",
    "\n",
    "To run this notebook, go to **Cell -> Run All**. Read the comments in the notebook and inspect the output of each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4a664",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, you will create a Q&A application using Bedrock. Following is the flow.\n",
    "\n",
    "1. Customer asks a question in natural language.\n",
    "2. Use Text-to-Text LLM to generate SQL query for the question. LLM will use the schema of the tables containing retail website data. This will be passed as one of the prompt input variables.\n",
    "3. Execute the query against the website data stored in Amazon RDS Postgres database.\n",
    "4. Use Text-to-Text LLM to interpret the query results in natural language.\n",
    "5. Display the answer to the question.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "![Code Generation](../images/sql-generation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dcf4cd",
   "metadata": {},
   "source": [
    "### Install required dependencies\n",
    "\n",
    "**Important:** You may see an error or a warning that \"you may need to restart the kernel\" from the following cell. **Ignore** and proceed with the next cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --no-build-isolation --upgrade \\\n",
    "    \"boto3==1.28.63\" \\\n",
    "    \"awscli==1.29.63\" \\\n",
    "    \"botocore==1.31.63\" \\\n",
    "    \"langchain==0.0.309\" \\\n",
    "    \"psycopg2-binary==2.9.9\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97e23c",
   "metadata": {},
   "source": [
    "<h3> Import required packages </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import psycopg2\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5637d89",
   "metadata": {},
   "source": [
    "<h3> Initialize Bedrock client </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11248a45",
   "metadata": {},
   "source": [
    "#### Get the name of the S3 bucket created from CloudFormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_bucket = None\n",
    "response = s3.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    if 'reinvent-retails-bucket' in bucket['Name']: \n",
    "        s3_bucket = bucket['Name']\n",
    "\n",
    "print(s3_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cea604",
   "metadata": {},
   "source": [
    "#### Read postgres database schema\n",
    "\n",
    "Typically, all the data from web applications are stored in a database. In our case, all the data from our re:Invent retails web application (like products, orders, customer reviews, product ratings etc.) are stored in RDS Postgres database. You can easily get the schema of the tables with the [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) tool. We have already taken a dump of the table schemas from the RDS database and uploaded it to S3. Let's read this schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128df429",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_response = s3.get_object(Bucket=s3_bucket, Key=\"data/schema-mysql.sql\")\n",
    "schema = s3_response['Body'].read().decode(\"utf-8\")\n",
    "\n",
    "# Printing schemas of all the tables used in our web application\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44cbe3c",
   "metadata": {},
   "source": [
    "### Prompt template\n",
    "\n",
    "<p>Using this prompt template we are asking the LLM to generate an SQL query based on the Postgres database schema. Notice that we pass 7 rules for the LLM to adhere to while constructing the query. Read the prompt instructions carefully. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efefb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "            Human: Create an Postgres SQL query for a retail website to answer the question keeping the following rules in mind: \n",
    "            \n",
    "            1. Database is implemented in Postgres SQL.\n",
    "            2. Follow the Postgres syntax carefully while generating the query.\n",
    "            3. Enclose the query in <query></query>. \n",
    "            4. Use \"like\" and upper() for string comparison on both left hand side and right hand side of the expression. For example, if the query contains \"jackets\", use \"where upper(product_name) like upper('%jacket%')\". \n",
    "            5. If the question is generic, like \"where is mount everest\" or \"who went to the moon first\", then do not generate any query in <query></query> and do not answer the question in any form. Instead, mention that the answer is not found in context.\n",
    "            6. If the question is not related to the schema, then do not generate any query in <query></query> and do not answer the question in any form. Instead, mention that the answer is not found in context.  \n",
    "            7. If the question is asked in a language other than English, convert the question into English before constructing the SQL query. The string and varchar fields stored in the database are always in English.  \n",
    "\n",
    "            <schema>\n",
    "                {schema}\n",
    "            </schema>\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Assistant:\n",
    "            \n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2993d23",
   "metadata": {},
   "source": [
    "#### Ask a sample question in natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7568ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many products do you have in your store?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bfe3d",
   "metadata": {},
   "source": [
    "#### Pass this question to prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88453155",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_vars = PromptTemplate(template=prompt_template, input_variables=[\"question\",\"schema\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2225767",
   "metadata": {},
   "source": [
    "#### Now lets call the Bedrock LLM to generate SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Claude Anthropic LLM\n",
    "llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock)\n",
    "\n",
    "# Pass question and postgres schema of the web application\n",
    "prompt = prompt_vars.format(question=question, schema=schema)\n",
    "\n",
    "# Print response\n",
    "llm_response = llm(prompt)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e6517",
   "metadata": {},
   "source": [
    "#### As you can see from the above llm_response, the query is embedded inside \\<query\\>\\<\\/query\\> tags. Let's create a function retrieve the query inside the \\<query\\> tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_strings_recursive(test_str, tag):\n",
    "    # finding the index of the first occurrence of the opening tag\n",
    "    start_idx = test_str.find(\"<\" + tag + \">\")\n",
    " \n",
    "    # base case\n",
    "    if start_idx == -1:\n",
    "        return []\n",
    " \n",
    "    # extracting the string between the opening and closing tags\n",
    "    end_idx = test_str.find(\"</\" + tag + \">\", start_idx)\n",
    "    res = [test_str[start_idx+len(tag)+2:end_idx]]\n",
    " \n",
    "    # recursive call to extract strings after the current tag\n",
    "    res += extract_strings_recursive(test_str[end_idx+len(tag)+3:], tag)\n",
    " \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a46879",
   "metadata": {},
   "source": [
    "#### Get the query using the extract_strings_recursive function we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a003149",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "\n",
    "if \"<query>\".upper() not in llm_response.upper():\n",
    "    is_query_generated  = False\n",
    "else:\n",
    "    is_query_generated  = True\n",
    "    query = extract_strings_recursive(llm_response, \"query\")[0]\n",
    "    print(\"Query generated by LLM: \\n\" +query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ccd331",
   "metadata": {},
   "source": [
    "### Execute the query\n",
    "\n",
    "Now, let's connect to the RDS database and run this LLM-generated query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize secrets manager\n",
    "secrets = boto3.client('secretsmanager')\n",
    "\n",
    "sm_response = secrets.get_secret_value(SecretId='postgresdb-secrets')\n",
    "\n",
    "database_secrets = json.loads(sm_response['SecretString'])\n",
    "\n",
    "# Get secrets from the secrets manager\n",
    "dbhost = database_secrets['host']\n",
    "dbport = database_secrets['port']\n",
    "dbuser = database_secrets['username']\n",
    "dbpass = database_secrets['password']\n",
    "dbname = database_secrets['name']\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "dbconn = psycopg2.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, database=dbname, connect_timeout=10)\n",
    "dbconn.set_session(autocommit=True)\n",
    "cursor = dbconn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297862cb",
   "metadata": {},
   "source": [
    "#### Execute the LLM-generated query in Amazon RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61273a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the extracted query\n",
    "cursor.execute(query)\n",
    "\n",
    "query_result = cursor.fetchall()\n",
    "dbconn.close()\n",
    "\n",
    "resultset = ''\n",
    "if len(query_result) > 0:\n",
    "    for x in query_result:\n",
    "        resultset = resultset + ''.join(str(x)) + \"\\n\"\n",
    "\n",
    "print(\"Query result: \\n\" +resultset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b5251",
   "metadata": {},
   "source": [
    "#### Now, lets create another prompt template to interpret the query results. \n",
    "\n",
    "This prompt template defines rules while describing query result. This is the final result that will be seen by the user as an answer to their question. Idea is to derive natural language answer for a natural language question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "        Human: This is a Q&A application. We need to answer questions asked by the customer at an e-commerce store. \n",
    "        The question asked by the customer is {question}\n",
    "        \n",
    "        We ran an SQL query in our database to get the following result. \n",
    "\n",
    "        <resultset>\n",
    "            {resultset}\n",
    "        </resultset>\n",
    "\n",
    "        Summarize the above result and answer the question asked by the customer keeping the following rules in mind: \n",
    "        \n",
    "        1. Don't make up answers if <resultset></resultset> is empty or none. Instead, answer that the item is not available based on the question.\n",
    "        2. Mask the PIIs phone, email and address if found the answer with \"<PII masked>\"\n",
    "        3. Don't say \"based on the output\" or \"based on the query\" or \"based on the question\" or something similar.  \n",
    "        4. Keep the answer concise. \n",
    "        5. Don't give an impression to the customer that a query was run. Instead, answer naturally. \n",
    "\n",
    "        Assistant:\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008cfaf",
   "metadata": {},
   "source": [
    "#### Create prompt and invoke LLM to interpret results retrieved from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c07b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_vars = PromptTemplate(template=prompt_template, input_variables=[\"question\",\"resultset\"])\n",
    "\n",
    "# Pass question and result set to prompt template\n",
    "prompt = prompt_vars.format(question=question, resultset=resultset)\n",
    "\n",
    "# Call LLM to interpret query result\n",
    "describe_query_result = llm(prompt)\n",
    "\n",
    "print(\"Question asked by the user: \\n\")\n",
    "print(question + \"\\n\\n\")\n",
    "print(\"Answer interpreted by LLM: \\n\")\n",
    "print_ww(describe_query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d99689",
   "metadata": {},
   "source": [
    "<h3> You've successfully created a Q&A application using SQL generation with Amazon Bedrock!</h3>\n",
    "\n",
    "Please stop the notebook kernel before proceeding with **Kernel -> Interrupt**.\n",
    "\n",
    "#### Now, let's integrate this feature into our retail web application. Please go back to Workshop Studio and follow the instructions to build this feature using your Cloud9 IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9ad10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
